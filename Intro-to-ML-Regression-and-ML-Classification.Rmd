---
title: "Intro-to-ML-Regression-and-ML-Classification"
author: "Reinp"
date: "`r Sys.Date()`"
output:
  pdf_document: default
  html_document: 
    keep_md: yes
  word_document: default
---

# R Programming: Linear Regression (OLS)

## Set Chunk requirements

```{r setup, include=TRUE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
#echo=FALSE indicates that the code will not be shown in the final document 
#(though any results/output would still be displayed).
#include=FALSE to have the chunk evaluated, but neither the code nor its output displayed
# warning=FALSE and message=FALSE suppress any R warnings or messages from being included 
#in the final document
```

## Load Relevant Packages and Data Set

```{r}
setwd('E:/Documents/Reinp/GitHub Respositories/Intro-to-ML-Regression-and-ML-Classification')

library(stats)
library(psych)
library(tidyverse)
## tidyverse includes readr, ggplot2, dplyr, forcats, tibble, tidyr, purrr, stringr

cars20_tr <- read_csv("data/cars20_train.csv")

view(cars20_tr)

```

## Exploratory Data Analysis

### Exploratory plots in base R 

```{r}

### Histogram of all MPGs

hist(cars20_tr$mpg, breaks = 30)

### Scatter plot of mpg vs disp 

plot(mpg~disp, data = cars20_tr)


### Box plots mpg vs transmission 
boxplot(mpg~transmission, data = cars20_tr)


```

### Exploratory plots in ggplot2

```{r}
### What is the distribution of mpg?

mpg_hist <- ggplot(data = cars20_tr, 
       mapping = aes(
         x = mpg
       ) ) + geom_histogram(fill = "lightblue", colour = "black") +
  theme_bw()

mpg_hist

### What is the relationship between displacement and mpg?

displ_mpg_Scattplt <- ggplot(data = cars20_tr,
       mapping = aes(
         x = disp,
         y = mpg
       )) + geom_point(alpha = 0.3) + 
  geom_smooth()

displ_mpg_Scattplt

### What is the relationship between transmission type and mpg?

trans_mpg_boxplt <- ggplot(data = cars20_tr,
       aes(x = transmission, 
           y = mpg)) + 
  geom_boxplot() + theme_minimal()

trans_mpg_boxplt
```

## Simple Linear Regression

```{r}
## Simple 1 variable model using displacement to predict mpg


model1 <- lm(mpg~disp, data = cars20_tr)

summary(model1)

## To get the model predicted values, use the predict function. 
## It will output a vector of values: one for each car in the training data.

cars20_tr$predictmodel1 <- predict(model1, newdata = cars20_tr)

# predict(model1, newdata = cars20_tr, interval = "confidence")

cars20_tr$residualmodel1 <- cars20_tr$predictmodel1 - cars20_tr$mpg


cars20_tr <- cars20_tr %>% mutate(model1 = predict(model1, newdata = cars20_tr),
                                            residual1 = model1 - mpg)

```
### assess the model fit

```{r}

#plot(model1) ### shows several diagnostic graphs

plot(model1, 1)
plot(model1, 2)
plot(model1, 3)  
plot(model1, 4)  
plot(model1, 5)  
plot(model1, 6)

### Showing the fitted model vs the actual mpg values

ggplot(data = cars20_tr, 
       mapping = aes(
        x = mpg,
        y = predictmodel1
       )
) + 
  geom_point() + 
  geom_abline(slope = 1, intercept = 0, colour = "red") + 
  xlim(c(0,60)) + ylim(c(0,60))

### Showing the residual vs the actual mpg values

ggplot(
  data = cars20_tr,
  mapping = aes(
    x = mpg,
    y = residualmodel1
  )
) + 
  geom_point() + 
  geom_hline(yintercept = 0, linetype = "dashed", colour = "red")
```

### Testing on the query set

```{r}
cars20_qu <- read_csv("data/cars20_query.csv")
view(cars20_qu)

cars20_qu$predictmodel1 <- predict(model1, newdata = cars20_qu)
cars20_qu$residualmodel1 <- cars20_qu$predictmodel1 - cars20_qu$mpg


ggplot(data = cars20_qu, 
       mapping = aes(
        x = mpg,
        y = predictmodel1
       )
) + 
  geom_point() + 
  geom_abline(slope = 1, intercept = 0, colour = "blue") + 
  xlim(c(0,60)) + ylim(c(0,60))


ggplot(
  data = cars20_qu,
  mapping = aes(
    x = mpg,
    y = residualmodel1
  )
) + 
  geom_point() + 
  geom_hline(yintercept = 0, linetype = "dashed",  colour = "blue")

```

## Multiple linear regression

```{r}
model2 <- lm(mpg~disp+transmission+drive+atvType, data = cars20_tr) 
model2

summary(model2)

cars20_tr$model2 <- predict(model2, newdata = cars20_tr)

cars20_tr$residual2 <- cars20_tr$model2 - cars20_tr$mpg
```
### assess the model fit

```{r}
plot(model2)


ggplot(data = cars20_tr, 
       mapping = aes(
        x = mpg,
        y = model2
       )
) + 
  geom_point() + 
  geom_abline(slope = 1, intercept = 0, colour = "green") + 
  xlim(c(0,60)) + ylim(c(0,60))


ggplot(
  data = cars20_tr,
  mapping = aes(
    x = mpg,
    y = residual2
  )
) + 
  geom_point() + geom_hline(yintercept = 0, linetype = "dashed", colour = "green")
```

### Testing on the query set

```{r}

cars20_qu$predictmodel2 <- predict(model2, newdata = cars20_qu)
cars20_qu$residualmodel2 <- cars20_qu$predictmodel2 - cars20_qu$mpg


ggplot(data = cars20_qu, 
       mapping = aes(
        x = mpg,
        y = predictmodel2
       )
) + 
  geom_point() + 
  geom_abline(slope = 1, intercept = 0, colour = "yellow") + 
  xlim(c(0,60)) + ylim(c(0,60))


ggplot(
  data = cars20_qu,
  mapping = aes(
    x = mpg,
    y = residualmodel2
  )
) + 
  geom_point() + 
  geom_hline(yintercept = 0, linetype = "dashed",  colour = "yellow")

```




# R Programming: Regression (Decision Trees)

## Making a model with two variables

### Training data

```{r}
library(rpart) ### for decision tree 
library(rpart.plot) ## for showing decision tree

cars20_trdt <- read_csv("data/cars20_train.csv")
view(cars20_trdt)

dt1 <- rpart(mpg~disp+transmission, data = cars20_trdt)
dt1

rpart.plot(dt1)

cars20_trdt$predict1 <- predict(dt1, newdata = cars20_trdt)
cars20_trdt$residual1 <- cars20_trdt$predict1 - cars20_trdt$mpg

## Comparing model values to actual values

ggplot(data = cars20_trdt,
       aes(x = mpg,
           y = predict1)) + 
  geom_point(shape = 21) + 
  xlim(c(0,60)) + 
  ylim(c(0,60)) + 
  geom_abline(slope = 1, intercept = 0, colour = "orange") + theme_bw() + 
  ggtitle("Predict Training set")

## Comparing residual values to actual values

ggplot(data = cars20_trdt,
  aes(x = mpg,
    y = residual1)) + 
  geom_point() + 
  geom_hline(yintercept = 0, linetype = "dashed", colour = "orange") +
  ggtitle("Residual Training set")
```

### Query data

```{r}
cars20_qudt <- read_csv("data/cars20_query.csv")
view(cars20_qudt)

cars20_qudt$predict1 <- predict(dt1, newdata = cars20_qudt)
cars20_qudt$residual1 <- cars20_qudt$predict1 - cars20_qudt$mpg


ggplot(data = cars20_qudt,
       aes(x = mpg,
           y = predict1)) + 
  geom_point(shape = 21) + theme_bw() + 
  xlim(c(0,60)) + ylim(c(0,60)) + 
  geom_abline(slope = 1, intercept = 0) +
  ggtitle("Predict Query set")


ggplot(data = cars20_qudt,
  aes(x = mpg,
    y = residual1)) + 
  geom_point() + 
  geom_hline(yintercept = 0, linetype = "dashed") +
  ggtitle("Residual Query set")
```

## Making a model with all available 

### Training data

```{r}
## create a data set without the make and model variables using dplyr::select

cars20_trdt2 <- read_csv("data/cars20_train.csv") %>%
  select(-make, -model)
view(cars20_trdt2)


dt2 <- rpart(mpg~., data = cars20_trdt2)
rpart.plot(dt2)


cars20_trdt2$predict1 <- predict(dt2, newdata = cars20_trdt2)
cars20_trdt2$residual1 <- cars20_trdt2$predict1 - cars20_trdt2$mpg

ggplot(data = cars20_trdt2,
       aes(x = mpg,
           y = predict1)) + 
  geom_point() + 
  xlim(c(0,60)) + 
  ylim(c(0,60)) + 
  geom_abline(slope = 1, intercept = 0, colour = "purple") + theme_bw() + 
  ggtitle("Predict Training set")



ggplot(data = cars20_trdt2,
  aes(x = mpg,
    y = residual1)) + 
  geom_point() + 
  geom_hline(yintercept = 0, linetype = "dashed", colour = "purple") +
  ggtitle("Residual Training set")

```

### Query data

```{r}

cars20_qudt2 <- read_csv("data/cars20_query.csv")

cars20_qudt2 <- subset(cars20_qudt2, select = -c(make,model)) #Delete columns by name

cars20_qudt2 <- cars20_qudt2[ !(cars20_qudt2$class == "Passenger Van"), ]
view(cars20_qudt2)

### Query data has a factor (class) with new level (Passenger Van) not in train data
#1. delete the Passenger Van rows from query data set and model will run perfectly

#2. You could try updating dt2$xlevels[["class"]] in the model object
# dt2$xlevels[["class"]] <- union(dt2$xlevels[["class"]], levels(cars20_qudt2$class))

#3. exclude (but not remove) "class" from the training data
#dt2 <- rpart(mpg~., data=cars20_trdt2[,!colnames(cars20_trdt2) %in% c("class")])


cars20_qudt2$predict1 <- predict(dt2, newdata = cars20_qudt2)
cars20_qudt2$residual1 <- cars20_qudt2$predict1 - cars20_qudt2$mpg


ggplot(data = cars20_qudt2,
       aes(x = mpg,
           y = predict1)) + 
  geom_point() + 
  xlim(c(0,60)) + 
  ylim(c(0,60)) + 
  geom_abline(slope = 1, intercept = 0, colour = "brown") + theme_bw() + 
  ggtitle("Predict Training set")



ggplot(data = cars20_qudt2,
  aes(x = mpg,
    y = residual1)) + 
  geom_point() + 
  geom_hline(yintercept = 0, linetype = "dashed", colour = "brown") +
  ggtitle("Residual Training set")

```

# R Programming: Classification

## Load Data Set

```{r}
stackoverflow <- read_csv("data2/stackoverflow.csv")

view(stackoverflow)

# Print stackoverflow
glimpse(stackoverflow)

str(stackoverflow)

```

## Exploring the Stack Overflow Survey

```{r}
count(stackoverflow, Remote, sort = TRUE)

count(stackoverflow, Country, sort = TRUE)

#Dolar sign Syntax
table(stackoverflow$Remote)
table(stackoverflow$Country)


ggplot(stackoverflow, 
       aes(x = Remote, y = YearsCodedJob)) +
    geom_boxplot() +
    labs(x = NULL,
         y = "Years of professional coding experience")


ggplot(stackoverflow, 
       aes(x = Remote, y = Salary)) +
    geom_boxplot() +
    labs(x = NULL,
         y = "Salary")


ggplot(stackoverflow, 
       aes(x = Remote, y = CareerSatisfaction)) +
    geom_boxplot() +
    labs(x = NULL,
         y = "Career Sattisfaction")

```

## Create training and test sets

```{r}
## Remove the Respondent column

stackoverflow1 <- select(stackoverflow, -Respondent)

## convert the Remote variable to a 0-1 variable

stackoverflow2 <- mutate(stackoverflow1, Remote = ifelse(Remote == "Remote", 1,0))

## Create training and test sets (in a 80:20 ratio)

# Split the data into training and testing sets
set.seed(1234)
library(rsample)
stack_split <- initial_split(stackoverflow2, prop = 0.8)

stack_train <- training(stack_split)
stack_test <- testing(stack_split)
```

## Fit a simple model

### Build a simple logistic regression model

```{r}

simple_glm <- glm(Remote~., family = "binomial", data = stack_train)
simple_glm
summary(simple_glm)


#CountryIndia, CountryUnitedStates, Salary , YearsCodedJob, OpenSourceTRUE, 
#CompanySizeNumber, CareerSatisfaction, Database administrator`TRUE, 
#Desktop applications developer`TRUE are significant.


#The logistic regression coefficients give the change in the log odds of the outcome 
#for a one unit increase in the predictor variable.

#For a one unit increase in salary, the change in log odds of developers more likely 
#to work remotely increases by 0.000004632 holding all other variables constant.

#For a one unit increase in CompanySizeNumber, the change in log odds of developers more 
#likely to work remotely decreases by 0.00006976 holding all other variables constant.

#The log odds of being a Database administrator likely to work remotely is 0.3070 
#higher than the log odds of not being a Database administrator holding all other 
#variables constant.

#The log odds of being a Desktop applications developer likely to work remotely is 0.3062 
#lower than the log odds of not being a Desktop applications developer holding all other 
#variables constant.

```

### confidence intervals for the coefficient & odds Ratio

```{r}
### CIs using profiled log-likelihood
confint(simple_glm)

### CIs using standard errors
confint.default(simple_glm)

###exponentiate the coefficients and interpret them as odds-ratios
## odds ratios only
exp(coef(simple_glm))

#column-wise.
## odds ratios and 95% CI
exp(cbind(OR = coef(simple_glm), confint(simple_glm)))


#For every one unit increase in Salary, the odds of developers more likely 
#to work remotely increases by a factor of 1.00000463 holding all other variables 
#constant.


#For every one unit increase in CompanySizeNumber, the odds of developers more likely 
#to work remotely decreases by a factor of 0.99993024 holding all other variables 
#constant.


#The odds of being a Database administrator likely to work remotely is 1.35933849 times 
#the odds of not being a Database administrator holding all other variables constant.


#The odds of being a Desktop applications developer likely to work remotely is 0.73626227 
#times the odds of not being a Desktop applications developer holding all other variables
#constant.

```

### Assessing the model

```{r}
#We may also wish to see measures of how well our model fits. This can be particularly
#useful when comparing competing models.

#The output produced by summary(simple_glm) included indices of fit, including 
#the null and deviance residuals and the AIC.

#One measure of model fit is the significance of the overall model. This test asks
#whether the model with predictors fits significantly better than a model with just an
#intercept (i.e., a null model).

#The test statistic is the difference between the residual deviance for the model
#with predictors and the null model. The test statistic is distributed chi-squared
#with degrees of freedom equal to the differences in degrees of freedom between the
#current and the null model (i.e., the number of predictor variables in the model).


#To find the difference in deviance for the two models (i.e., the test statistic),
#we use the command:
with(simple_glm, null.deviance - deviance)

#The degrees of freedom for the difference between the two models is equal to the number of
#predictor variables in the model, and can be obtained using:
with(simple_glm, df.null - df.residual)

#the p-value can be obtained using:
with(simple_glm, pchisq(null.deviance - deviance, df.null - df.residual, lower.tail = FALSE))

#To see the modelâ€™s log likelihood, we type:
logLik(simple_glm)

#(The Residual deviance is -2*log likelihood)
#-2*-1710.67=3421.34


#The chi-square of 274.8713 with 23 degrees of freedom and an associated p-value of less
#than 0.001 (5.26266e-45) tells us that our model as a whole fits significantly better
#than an empty model. This is sometimes called a likelihood ratio test.
```

### Removing unsignificant variables from the model

```{r}

simple_glm1 <- glm(Remote~. - Hobby - `Data scientist`-
                     `Developer with stats/math background` - DevOps -
                     `Embedded developer` - `Graphic designer` -
                     `Graphics programming` - `Machine learning specialist` -
                     `Mobile developer` - `Quality assurance engineer` -
                     `Systems administrator` - `Web developer` , family = "binomial",
                   data = stack_train)
simple_glm1
summary(simple_glm1)
```

### Predicting the model

```{r}

#Add the prediction as a column to the training set
stack_train$Predictsimple_glm <- predict(simple_glm, newdata = stack_train,
                                     type = "response")

stack_train$Predictsimple_glm1 <- predict(simple_glm1, newdata = stack_train,
                                     type = "response")

```

### Visualising model predicted values

```{r}
ggplot(stack_train, aes(x = Predictsimple_glm)) + 
  geom_histogram(fill = "skyblue", colour = "black") + facet_wrap(~Remote, ncol = 1, 
                                scales = "free_y")


ggplot(stack_train, aes(x = Predictsimple_glm1)) + 
  geom_histogram(fill = "magenta", colour = "grey") + facet_wrap(~Remote, ncol = 1, 
                                scales = "free_y")

ggplot(stack_train, aes(x = Predictsimple_glm)) + 
  geom_density(colour = "red") + facet_wrap(~Remote, ncol = 1, 
                                scales = "free_y")

ggplot(stack_train, aes(x = Predictsimple_glm1)) + 
  geom_density(colour = "green") + facet_wrap(~Remote, ncol = 1, 
                                scales = "free_y")

```

## A decision tree version

```{r}
stack_train2 <- training(stack_split)

library(rpart)
simple_tree <- rpart(Remote~., data = stack_train2)

simple_tree

library(rpart.plot)

rpart.plot(simple_tree)
```







